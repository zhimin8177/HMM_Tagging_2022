机器 学习 与 计算力学 的 结合 及 应用 初探
摘要 自 20 世纪 50 年代 以来 , 随着 计算机 科学 的 不断 进步 , 机器 学习 和 数据 科学 得到 了 长足 发展 . 这些 技术 一般 依靠 大量 数据 作 支撑 , 通过 训练 过程 提取 出 蕴藏 在 数据 内部 的 抽象 映射 关系 , 目前 已 被 成功 应用 于 化学 、 生物 等 自然科学 研究 领域 . 近年来 , 这些 技术 也 逐渐 受到 计算力学 领域 研究者 的 关注 . 本文 结合 作者 的 相关 研究 成果 介绍 了 机器 学习 、 数据 科学 与 计算力学 相 结合 的 3 种 形式 : 第一 种 是 与 有限元 方程 求解 方面 的 结合 , 直接 应用 卷 积神经 网络 算法 求解线性 有限元 方程 ; 第二 种 方式 结合 有限元 计算 和 机器 学习 预测 复杂 材料 结构 与 力学 性能 的 关系 . 本文 作者 应用 该 方法 基于 细 观页岩 扫描 照片 和 随机 建模 算法 , 成功 训练 出 可以 有效 预测 细 观页岩 样本 等 效模量 的 卷积神经 网络 ; 第三 种 方式 是 建立 基于 数据 驱动 的 计算力学 方法 , 比如 直接 利用 真实 的 材料 实验 数据 代替 材料 本构 模型 . 这些 工作 显示 了 机器 学习 、 数据 驱动 在 处理 材料 的 力学 实验 数据 、 设计 新型 材料 以及 创建 更 高效 的 计算力学 模型 方面 的 广阔 前景 . 随着 计算力学 的 发展 , 未来 将 可能 出现 更 多 将 数据 科学 、 机器 学习 与 计算力学 相 结合 的 应用 场景 , 进一步 开发 出 更加 强健 、 高效 和 保真 的 计算力学 方法 . 关键词 机器 学习 , 数据 驱动 , 人工 神经 网络 , 计算力学 , 有限 单元法 在 力学 发展 过程 中 , 研究者 往往 通过 提出 理想化 的 力学 模型 开展 理论 分析 . 这些 理想 模型 一般 具有 较为 规则 的 几何 形状 、 边界 条件 和 基于 显式 表达式 描述 的 材料 响应 函数 , 将 真实 的 复杂力学 问题 抽象 为 简单 的 数学 模型 , 进而 寻求 问题 的 解析 解 . 计算力学 方法 的 出现 使 求解 具有 更 复杂 的 几何 模型 和 边界 条件 的 问题 成为 可能 . 例如 , 有限 单元法 将 连续 求解 区域 划分 为 离散 单元 , 将 求解 连续 模型 上 的 偏微 分 方程 转换 为 求解 离散 模型 上 的 积分 方程 , 通过 离散 单元 上 的 近似 解逼 近 连续 模型 的 真实 解 [ 1 , 2 ] . 随着 计算机 科学 的 不断 发展 , 机器 学习 和 数据 科学 取得 了 长足进步 . 常见 的 一些 机器 学习 方法 包括 感知器 [ 3 ] 、 遗传 编程 [ 4 ] 、 蒙特卡罗 方法 [ 5 ] 、 K- 近邻 算法 ( K-nearest neighbor , K-NN ) [ 6 ] 、 线性 回归 [ 7 ] 、 逻辑 回归 [ 8 , 9 ] 、 自 适应 增强 [ 10 ] 和 支持 向 量机 [ 11 , 12 ] 等 . 这些 方法 各具特点 . 例如 : 线性 回归 方法 使用 线性 函数 对 数据集 进行 拟合 , 通过 梯度 下降法 优化 最 小 二 乘 误差函数 , 一般 采用 均方 差函数 描述 误差 . 该 方法 计算 简单 且 易于 实现 , 但 其 固有 缺点 是 无法 拟合 非线性 数据 [ 7 ] . 逻辑 回归 是 一 种 用于 解决 分类 问题 的 判别式 模型 , 它 与 线性 回归 相比 最 大 的 区别 在于 引入 非线性 函数 , 不 要求 训练 数据 满足 线性 关系 , 一般 使用 分类 交叉熵 作为 误差 函 数 . 其 缺点 是 对于 多 分类 问题 的 拟合 精度 欠佳 [ 8 , 9 ] . K- 近邻 算法 基于 特征 空间 内 与 目标 样本 最 相似 ( 或 最 临近 ) 的 k 个 样本 的 类别 对 目标 样本 进行 分类 [ 13 ] . 该 方法 需要 记录 所有 训练 数据 , 导致 计算 成本 较 高 , 且 结果 比较 依赖 于 k 的 取值 [ 14 ] . 但 该 方法 同时 拥有 以下 优点 : 简单 易 用 、 准确度 高且 可以 用于 分类 和 回归 问题 . 支持 向 量机 的 目标 是 在 高维度 特征 空间 寻找 最 优 的 划分 超平面 . 该 方法 适用 于 处理 高维度 样本 数据 , 被 广泛 应用 于 文本 分类 和 图像 识别 领域 [ 15 ] . 其 性能 依赖 于 核函数 的 选取 [ 16 ] , 另外 对于 大量 样本 的 计算 性能 还 有待 提高 [ 17 ] . 自 适应 增强 方法 在 迭代 训练 时 关注 之前 分类 错误 的 样本 , 通过 不断 修正 之前 的 错误 分类 结果 , 最终 将 每个 迭代步 训练 得到 的 弱 分类器 基于 权重 叠加 为 一个 强 分类器 . 该 方法 的 分类 精度 高且 不 容易 过 拟合 , 但 缺点 是 模型 训练 时间 较 长 [ 18 ] . 除 了 前 述 的 各 种 方 法 之 外 , 人 工 神 经 网 络 ( artificial neural network , ANN ) 已经 逐步 发展 成 机器 学习 领域 的 一个 重要 分 支 . 其 相关 研究 工作 可以 追溯 到 20 世纪 40 年代 . 1943年 , McCulloch 和 Pitts [ 19 ] 受到 生物 神经活性 特点 的 启发 , 首 次 提出 了 人工 神经元 的 数 学 模 型 . 1958年 , Rosenblatt [ 3 ] 建 立 了 感 知 器 ( perceptron ) 的 模型 , 该 模型 也 被 普遍 认为 是 现代 人工 神经 网络 的 前身 . 最 早 的 人工 神经 网络 模型 是 由 斯坦福 大学 创建 的 名 为 “ ADALINE ” 和 “ MADALINE ” [ 20 ] . 这些 模型 受制于其线性 和 相对 简单 的 结构 , 难以 被 用于 解决 复杂 非线性 问题 . 1971年 , Ivakhnenko [ 21 , 22 ] 首 次 提出 了 基于 感知器 的 多 层 网络 模型 , 并 在 模型 中 应用 了 非线性 多 项式 函 数 . 20 世纪 70 ~ 80 年代 , Ru-melhart 等 人 [ 23 ] 和 Werbos [ 24 ] 分别 独立 地 提出 了 反向 传播 算法 . 该 算法 后来 逐步 发展 为 现代 深度 神经 网络 的 核心 组成部分 . 2006年 , Hinton 和 Salakhutdinov [ 25 ] 提 出 了 基 于 受 限 玻 尔 兹 曼 机 ( restricted Boltzmann machine , RBM ) 的 神经 网络 模型 且 首 次 明确 地 提出 深度 学习 的 概念 . 该 模型 通过 训练 包括 多 个 中间 层 的 神经 网络 , 可以 有效 的 提取 高 维度 数据 样本 的 特征 , 实现 了 数据 从 高维度 到 低 维度 的 变换 . 对比 上述 各种 机器 学习 方法 , 人工 神经 网络 模型 使用 了 非线性 激活 函数 , 利用 多 层 网络 结构 引入 了 大量 可 调节 参数 , 通过 梯度 下降 算法 挖掘 蕴含 于 大量 训练 数据 中 的 非线性 关系 , 一般 可 获得 良好 的 分类 和 回归 精度 , 且 对 噪音 数据 不 敏感 , 具有 一定 的 概括 和 推广 能力 [ 26 ~ 28 ] . 由于 这些 优点 , 人工 神经 网络 被 逐步 应用 到 信号 处理 、 模式 识别 、 自动 控制 、 生物 医药 和 金融 等 领域 [ 29 ] . 但是 , 多 层 神经 网络 结构 中 包含 大量 待定 参数 , 往往 需要 长 时间 的 训练 过程 . 网 络 结 构 的 选 择 依 赖 于 经 验 , 且 存 在 过 拟 合 问题 [ 30 , 31 ] . 另外 , 人工 神经 网络 模型 的 复杂 结构 使得 其 解释性 较 差 , 难以 直接 通过 模型 参数 解读 数据 背后 蕴藏 的 抽象 规律 . 不过 , 近年来 发展 的 反 卷积 网络 等 技术 可以 将 网络 中 的 低维 抽象 数据 映射 到 对应 的 高维 空间 进行 分析 [ 32 ~ 34 ] . 近年来 , 包括 人工 神经 网络 在内 的 各种 机器 学习 方法 被 不断 应用 于 工程 和 力学 等 领域 . Jin 等 人 [ 35 ] , Zhou 等 人 [ 36 ] , Fan [ 37 ] , Ge [ 38 ] , Feng 和 Diao [ 39 ] 将 人工 神经 网络 用于 岩石力学 的 研究 ; Wu 和 Zhao [ 40 ] , Gao 和 Li [ 41 ] , Zhang 和 Zhang [ 42 ] 利用 人工 神经 网络 进行 结构 优化 设计 ; Xia 和 Xiong [ 43 ] , Feng 和 Wang [ 44 ] , Chen 等 人 [ 45 ] 基于 人工 神经 网络 开展 了 边坡 稳定性 的 相关 研究 ; Lei 等 人 [ 46 ] , Ohsaki [ 47 ] , Hajela 和 Lee [ 48 ] 将 支持 向量 回归 、 K 近邻 算法 等 模型 用于 拓扑 优化 设计 ; Gha-boussi 等 人 [ 49 ] , Jung 和 Ghaboussi [ 50 ] , Ji 等 人 [ 51 ] , Fu-rukawa 和 Yagawa [ 52 ] , Hashash 等 人 [ 53 ] , Sun 等 人 [ 54 ] 利用 人工 神经 网络 研究 了 固体 材料 的 本构 关系 ; Faller 和 Schreck [ 55 ] , Wang 和 Liao [ 56 ] , Yuhong 和 Wenxin [ 57 ] , Butz 和 Stryk [ 58 ] , Beigzadeh 和 Rahimi [ 59 ] , Mi 等 人 [ 60 ] 借助 数值 计算 方法 和 人工 神经 网络 开展 了 流体力学 的 相关 研究 . 这些 研究 工作 表明 , 机器 学习 模型 在 处理 材料 的 力学 响应 数据 、 揭示 复杂 或 模糊 的 力学 机理 等 方面 具有 潜在 优势 , 逐渐 受到 计算力学 研究者 的 关注 . 本文 将 通过 3 个 例子 来 初步 阐述 数据 科学 、 机器 学习 等 与 计算力学 结合 的 应用 , 并 希望 为 该 领域 的 进一步 发展 提供 新 的 思路 . 1 卷 积神经 网络 在 线性 有限元 求解 中 的 应用 人工 神经 网络 因 其 优良 的 数据 拟合 性能 , 逐渐 成为 目前 主流 的 机器 学习 技术 之一 . 其 核心 思想 是 构建 一个 包括 大量 待定 参数 的 网络结构 , 通过 梯度 下降 算法 寻找 最 优 的 网络 参数 组合 , 使得 误差 函数 达到 极 值 . 学者 们 尝试 将 人工 神经 网络 的 算法 应用 于 线弹性 有限元 方程 和 偏微分 方程 的 求解 中 [ 61 ~ 64 ] . 在 本节 中 , 作者 将 目前 广泛 应用 的 卷积神经 网络 框架 整合 于 线弹性 有限元 的 求解 过程 中 . 首先 简要 回顾 传统 人工 神经 网络 的 基本 原理 , 然后 将 其 与 卷积神经 网络 结构 进行 对比 , 接着 阐述 本节 所 提出 方法 的 实现 过程 . 图 1 ( a ) 是 人工 神经 网络 基本 结构 的 示意图 . 人工 神经 网络 中 一般 包括 一个 输入层 、 一个 输出层 和 若干 中间 隐层 . 网络 中 的 每 一个 圆 代表 一个 神经 元 . 神经元 的 值 称为 “ 激活 ” . 两 个 神经元 之间 的 连接线 称为 3 评 述 图 1 ( 网络版 彩色 ) 人工 神经 网络 结构 示意 图 . ( a ) 传统 神经 网络 ; ( b ) 卷积神经 网络 权 重 . 神经元 的 值 由 上 一 层 神经元 的 值 与 两 层 神经元 之间 的 权重 计算 得到 [ 65 , 66 ] , 如式 ( 1 ) 所 示 . σ 代表 神经元 的 激活 , w 是 权重 , b 称为 偏差 . f 表示 非线性 函数 , 常用 的 非线性 函数 包括 ReLU 和 Sigmoid 函数 [ 66 , 67 ] : ( 1 ) 卷 积 神 经 网 络 ( convolutional neural network , CNN ) 是 人工 神经 网络 一个 重要 的 分支 或 变种 , 概念源 自 Fukushima 和 Miyake [ 68 ] 提 出 的 : “ 新 认 知机 ” ( neocognitron ) 模 型 . 20 世 纪 90年 代 , LeCun 等 人 [ 69 , 70 ] 确立 了 卷积神经 网络 的 现代 结构 , 并 不断 对 其 完善 . 与 传统 人工 神经 网络 相比 , 卷积神经 网络 具有 以下 主要 特点 [ 26 , 66 , 68 ] : 第一 个 特点 称为 共享 参数 , 表示 同 一个 卷积层 的 所有 神经元 拥有 相同 的 权重 w 和 偏差 b , 能够 有效 减少 网络 参数 数量 , 提高 训练 效率 . 第二 个 特点 是 局部 接受 域 . 图 1 ( a ) 所 示 的 传统 神经 网络 中 , 相邻 两 层 网络 中 的 神经元 是 完全 连接 的 . 而 在 如 图 1 ( b ) 所 示卷 积神经 网络 中 , 每 一 层 只有 部分 神经元 与 下 一 层 的 一个 神经元 相连 . 第三 个 特点 是 引入 池化 层 . 池化层 通常 施加 在 卷积层 之后 . 最 大 池 和 平 均 池 被 广 泛 用 于 简 化 来 自 卷 积 层 的 输 出 信息 [ 66 , 67 ] . 该 网络 被 成功 运用 在 机器 视觉 、 语音 处理 及 棋类 游戏 等 领域 [ 26 , 71 ~ 74 ] . 卷积神经 网络 的 神经 元 激活 通过 下式 得到 。 其中 , w 是 权重 , b 称为 偏差 , m 是 权重 的 尺寸 . w 与 b 共同 构成 网络 的 参数 集合 v. 输出层 神经元 的 值 与 数据 标签 之间 往往 存在 一定 的 误差 . 为 方便 阐述 , 本文 选择 均方 差函数 作为 误差函数 式 中 的 o 代表 输出层 的 神经 元 激活 , t 代表 数据 的 标签 , n 代表 训练 样本 的 数量 . 误差 函数 C ( v ) 的 变化 可以 通过 自变量 v 的 变化 来 近似 : ( 4 ) 其中 , 是 误差 函数 的 梯度 , 是 参数 的 微小 变化 量 . 的 取值 由 下式 确定 : 